<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>polus.data API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>polus.data</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import pickle
import random
import json
import inspect
from polus.core import BaseLogger, find_dtype_and_shapes
import tensorflow as tf
from transformers import TFAutoModel
from transformers.modeling_tf_outputs import TFBaseModelOutputWithPooling
from functools import wraps
from timeit import default_timer as timer

class IAccelerated_Map(BaseLogger):
    def __init__(self):
        super().__init__()
        
        self.__name__ = self.__class__.__name__
        
        if self.__class__.__name__ == &#34;IAccelerated_Map&#34;:
            raise Exception(&#34;This is an interface that cannot be instantiated&#34;)
            
    def build(self):
        raise (&#34;build method was not implemented&#34;)


class DataLoader(BaseLogger):
    
    def __init__(self, 
                 source_generator,
                 accelerated_map_f = None,
                 accelerated_map_batch = 128,
                 show_progress = False):
        &#34;&#34;&#34;
        source_generator         - the source generator that feeds the data
        accelerated_map_f        - an optional transformation function that would be applied to 
                                   the source generator samples, this function will be executed in the best hardware that tf founds on the device
        
        &#34;&#34;&#34;
        super().__init__()
        
        self.show_progress = show_progress
        self.accelerated_map_batch = accelerated_map_batch
        self.accelerated_map_f = accelerated_map_f
        
        self.sample_generator = self._build_sample_generator(source_generator)

        dytpes, shapes = find_dtype_and_shapes(self.sample_generator())
        
        self.tf_dataset = tf.data.Dataset.from_generator(self.sample_generator, 
                                                         output_types= dytpes,
                                                         output_shapes= shapes)
            
        # expose all the tf dataset methods
        for method in filter(lambda x: not x[0].startswith(&#34;_&#34;), inspect.getmembers(self.tf_dataset, predicate=inspect.ismethod)):
            setattr(self, method[0], method[1])
    
    
    def _build_sample_generator(self, source_generator):
        
        # Logic to construct a generator that maybe applies the accelerated_map_f to the source_generator
        if self.accelerated_map_f is None:
            generator = source_generator
        
        else:
            # build and store the accelerated_map_f
            if isinstance(self.accelerated_map_f, IAccelerated_Map):
                # get the map function and run any heavy init only 1 time!
                self.accelerated_map_f = self.accelerated_map_f.build()
                        
            def generator():
                
                # BATCH = 128
                dytpes, shapes = find_dtype_and_shapes(source_generator())
                inner_tf_dataset = tf.data.Dataset.from_generator(source_generator, 
                                                                  output_types= dytpes,
                                                                  output_shapes= shapes)\
                                                  .batch(self.accelerated_map_batch)\
                                                  .prefetch(10)
                
                for i, data in enumerate(inner_tf_dataset):
                    
                    if self.show_progress:
                        print(f&#34;Iteration: {i*self.accelerated_map_batch}&#34;, end=&#34;\r&#34;)
                    
                    start_bert_t = timer()
                    data = self.accelerated_map_f(data)
                    self.logger.debug(f&#34;Time taken to run BERT {timer()-start_bert_t}&#34;)
                    ## debatching in python
                    ## TODO: this may be a bottleneck since python is slow
                    start_unbatch_t = timer()
                    if isinstance(data, dict):
                        key = list(data.keys())[0]
                        n_samples = data[key].shape[0] #batch size
                        for i in range(n_samples):
                            yield { k:v[i] for k, v in data.items() } 
                    elif isinstance(data, (list,tuple)):
                        n_samples = data[0].shape[0]
                        for i in range(n_samples):
                            yield [ v[i] for v in data ] 
                    else:
                        raise ValueError(f&#34;the accelerated_map_f function does not yield a dict nor tuple nor a list&#34;)
                    self.logger.debug(f&#34;Time taken to unbatch {timer()-start_unbatch_t}&#34;)
            
        return generator

    def __iter__(self):
        return self.tf_dataset.__iter__()

    def get_n_samples(self):
        if hasattr(self, &#34;n_samples&#34;):
            return self.n_samples
        else:
            self.logger.info(&#34;this dataset does not have the number of samples in cache so it will take some time to counting&#34;)
            n_samples = 0
            for _ in self.tf_dataset:
                n_samples += 1
                
            self.n_samples = n_samples
            
            return self.n_samples
        
        
        
class CachedDataLoader(DataLoader):
    
    
    def __init__(self, 
                 source_generator = None,
                 accelerated_map_f = None,
                 accelerated_map_batch = 128,
                 show_progress = False,
                 tf_sample_map_f = None, # sample mapping function written and executed in tensorflow that is applied before the data is stored in cache
                 py_sample_map_f = None, # sample mapping function written and executed in python that is applied before the data is stored in cache
                 do_clean_up = False,
                 cache_additional_identifier = &#34;&#34;,
                 cache_chunk_size = 8192,
                 cache_folder=os.path.join(&#34;.polus_cache&#34;,&#34;data&#34;),
                 cache_index = None): # this variable is used to init a CacheDataLoader with an already cached index usefull in merge
        
        # impossible condition
        assert source_generator is not None or cache_index is not None

        self.cache_folder = cache_folder
        self.cache_chunk_size = cache_chunk_size
        self.cache_additional_identifier = cache_additional_identifier
        self.shuffle_blocks = False
        self.do_clean_up = do_clean_up
        self.__tf_sample_map_f = tf_sample_map_f
        self.__py_sample_map_f = py_sample_map_f
        self.cache_index = cache_index
        
        # the parent DataLoader will call _build_sample_generator that contains the logic to build the dataloader
        try:
            super().__init__(source_generator, accelerated_map_f=accelerated_map_f, show_progress=show_progress, accelerated_map_batch=accelerated_map_batch)
        except Exception as e:
            # here we dont want to solve the exception, we just want to clean up de previously created files
            self.logger.info(&#34;An error has occured so all the created files will be deleted&#34;)
            self.clean()
            raise e
            
    @classmethod
    def from_cached_index(cls, index_path):
        
        index_info = cls.read_index(index_path)
        
        return cls(cache_index=index_info)
        
    
    @classmethod
    def merge(cls, *cache_dataloaders):
        assert (len(cache_dataloaders)&gt;1)

        index_info = {&#34;files&#34;:[],
                      &#34;cache_chunk_size&#34;: 0,
                      &#34;n_samples&#34;: 0
                      }
        
        # read files
        for dl in cache_dataloaders:
            
            index = CachedDataLoader.read_index(dl.cache_index_path)
                
            index_info[&#34;n_samples&#34;] += index[&#34;n_samples&#34;]
            index_info[&#34;files&#34;].extend(index[&#34;files&#34;])
            
            # this is a bit starange, but it is possible to have different DL with diff chunk size so we will pick the larger one to define the conjunt
            index_info[&#34;cache_chunk_size&#34;] = max(index_info[&#34;cache_chunk_size&#34;], index[&#34;cache_chunk_size&#34;])
            
        return cls(cache_index=index_info)
                 
    @staticmethod
    def read_index(file_path):
        with open(file_path, &#34;r&#34;) as f:
            index = json.load(f)
        return index
    
    def _build_sample_generator(self, source_generator):
        
        if self.cache_index is not None:
            # we are alredy have the data needed to create the DataLoader
            # So, let&#39;s build it
            return self.__build_generator_from_index()
                 
        if not os.path.exists(self.cache_folder):
            os.makedirs(self.cache_folder)

        # get path to cache
        self.cache_base_name = self.__build_cache_base_name(source_generator)
        self.cache_base_path = os.path.join(self.cache_folder, self.cache_base_name)
        self.cache_index_path = f&#34;{self.cache_base_path}.index&#34;
        
        generator = None
        
        if not os.path.exists(self.cache_index_path):
             # build a generator that reads the files from cache
            self.logger.info(f&#34;DataLoader will store the samples in {self.cache_base_path}, with a max_sample per file of {self.cache_chunk_size}, this may take a while&#34;)
            # there are no history of a previous generator so we must generate the samples once and then store in cache
            # normaly apply the accelerated_map_f to the source_generator
            generator = super()._build_sample_generator(source_generator)
            
            if self.__tf_sample_map_f is not None:
                tf_source_generator = generator
                
                if isinstance(self.__tf_sample_map_f, IAccelerated_Map):
                    # get the map function and run any heavy init only 1 time!
                    self.__tf_sample_map_f = self.__tf_sample_map_f.build()
                
                def generator():
                    
                    dytpes, shapes = find_dtype_and_shapes(tf_source_generator())
                    inner_tf_dataset = tf.data.Dataset.from_generator(tf_source_generator, 
                                                                      output_types= dytpes,
                                                                      output_shapes= shapes)\
                                                      .map(self.__tf_sample_map_f, num_parallel_calls=tf.data.AUTOTUNE)\
                                                      .prefetch(tf.data.AUTOTUNE)

                    for data in inner_tf_dataset:
                        yield data
            
            if self.__py_sample_map_f is not None:
                
                py_source_generator = generator
                
                if isinstance(self.__py_sample_map_f, IAccelerated_Map):
                    # get the map function and run any heavy init only 1 time!
                    self.__py_sample_map_f = self.__py_sample_map_f.build()
                
                def generator():
                    for data in py_source_generator():
                        yield self.__py_sample_map_f(data)
        else:
            self.logger.info(f&#34;We found a compatible cache file for this DataLoader&#34;)
            
        return self._build_cache_generator(generator)
    
    def write_index_file(self, index_info):
        
        with open(self.cache_index_path, &#34;w&#34;) as f:
            json.dump(index_info, f)
    
    def _build_cache_generator(self, generator = None):
        # first its need to store in cache the samples
        if generator is not None:
            
            index_info = {&#34;files&#34;:[],
                          &#34;cache_chunk_size&#34;: self.cache_chunk_size,
                          }
            
            def write_to_file(file_id, data, index):
                
                file_path = f&#34;{self.cache_base_path}_{file_id:04}.part&#34;

                with open(file_path, &#34;wb&#34;) as f:
                    pickle.dump(data, f)
                    
                index[&#34;files&#34;].append(file_path)
            
            _temp_data = []
            _file_index = 0
            n_samples = 0
            
            # TODO: Change the tf section here so that all the tf function launched here can be destroyed
            self.logger.info(&#34;Starting to cache the dataset, this may take a while&#34;)
            
            for data in generator():
                n_samples += 1
                
                _temp_data.append(data)
                if len(_temp_data) &gt;= self.cache_chunk_size:
                    # save data to file
                    
                    write_to_file(_file_index, _temp_data, index_info)
                    
                    # clean up 
                    _temp_data = []
                    _file_index+=1
            
            # TODO: swap to the main tf session and destroy the previous one
            
            if len(_temp_data)&gt;0:
                # save the reminder data
                write_to_file(_file_index, _temp_data, index_info)
            
            index_info[&#34;n_samples&#34;] = n_samples
            
            # write the index file
            self.write_index_file(index_info)
        
        if self.do_clean_up:
            # TODO: make test to see if this thing works
            self.logger.info(&#34;The current tf session will be reseted to clear the computation done by the mapping function&#34;)
            tf.keras.backend.clear_session()
        
        # read the index from file
        self.cache_index = self.__class__.read_index(self.cache_index_path)
        
        # Build the cache generator
        
        return self.__build_generator_from_index()

    
    def clean(self):
        if hasattr(self, &#34;cache_index&#34;) and self.cache_index is not None and &#34;files&#34; in self.cache_index:
            for file in self.cache_index[&#34;files&#34;]:
                if os.path.exists(file):
                    os.remove(file)
        if hasattr(self, &#34;cache_index_path&#34;) and self.cache_index_path is not None and os.path.exists(self.cache_index_path):
            os.remove(self.cache_index_path)
                 
    def __build_generator_from_index(self):
        # set n_samples
        self.n_samples = self.cache_index[&#34;n_samples&#34;]
        self.cache_chunk_size = self.cache_index[&#34;cache_chunk_size&#34;]
                 
        self.logger.info(f&#34;Total number of samples in dataset: {self.n_samples}&#34;)
        
        def generator():
            aux_file_index = list(range(len(self.cache_index[&#34;files&#34;])))
            
            if self.shuffle_blocks:
                random.shuffle(aux_file_index)
                
            for file_index in aux_file_index:
                with open(self.cache_index[&#34;files&#34;][file_index], &#34;rb&#34;) as f:
                    for sample in pickle.load(f):
                        yield sample
                 
        return generator
        
    def __build_cache_base_name(self, source_generator):
        name = &#34;&#34;
        if self.cache_additional_identifier!=&#34;&#34;:
            name = f&#34;{self.cache_additional_identifier}_&#34;
        
        if self.accelerated_map_f is not None:
            name += self.accelerated_map_f.__name__
            
        if self.__tf_sample_map_f is not None:
            name += self.__tf_sample_map_f.__name__
            
        if self.__py_sample_map_f is not None:
            name += self.__py_sample_map_f.__name__
            
        return f&#34;{name}_{source_generator.__name__}&#34;
    
    
    def pre_shuffle(self):
        &#34;&#34;&#34;
        The order of the cached files will be readed in a random order
        &#34;&#34;&#34;
        self.shuffle_blocks = True
        
        return self

    
class CachedDataLoaderwLookup(CachedDataLoader):
    &#34;&#34;&#34;
    Correspond to a CachedDataLoader but also keeps track of an aditional lookup file
    &#34;&#34;&#34;
    
    def __init__(self, 
                 lookup_data = None,
                 cache_index=None,  # this variable is used to init a CacheDataLoader with an already cached index usefull in merge
                 *args,
                 **kwargs,
                ):
        
        if cache_index is not None and &#34;lookup_file&#34; in cache_index:
            with open(cache_index[&#34;lookup_file&#34;], &#34;rb&#34;) as f:
                lookup_data = pickle.load(f)
        
        if lookup_data is None:
            raise ValueError(&#34;Do not use CachedDataLoaderwLookup without setting a lookup_data, instead use CachedDataLoader&#34;)
        
        self.lookup_data = lookup_data
        
        # the parent DataLoader will call _build_sample_generator that contains the logic to build the dataloader
        super().__init__(*args, cache_index=cache_index, **kwargs)
        
    def get_lookup_data(self):
        return self.lookup_data
    
    @staticmethod
    def _load_lookup_data(lookup_file):
        with open(lookup_file, &#34;rb&#34;) as f:
            return pickle.load(f)
    
    @classmethod
    def merge(cls, *cache_dataloaders):
        assert (len(cache_dataloaders)&gt;1)

        index_info = {&#34;files&#34;:[],
                      &#34;cache_chunk_size&#34;: 0,
                      &#34;n_samples&#34;: 0
                      }
        
        lookup_data = []
        
        # read files
        for dl in cache_dataloaders:
            
            index = CachedDataLoaderwLookup.read_index(dl.cache_index_path)
                
            index_info[&#34;n_samples&#34;] += index[&#34;n_samples&#34;]
            index_info[&#34;files&#34;].extend(index[&#34;files&#34;])
            
            # this is a bit starange, but it is possible to have different DL with diff chunk size so we will pick the larger one to define the conjunt
            index_info[&#34;cache_chunk_size&#34;] = max(index_info[&#34;cache_chunk_size&#34;], index[&#34;cache_chunk_size&#34;])
            
            lookup_data.extend(CachedDataLoaderwLookup._load_lookup_data(index[&#34;lookup_file&#34;]))
            
        return CachedDataLoaderwLookup(cache_index=index_info, lookup_data=lookup_data)
    
    def clean(self):

        if hasattr(self, &#34;cache_index&#34;) and self.cache_index is not None and &#34;lookup_file&#34; in self.cache_index and os.path.exists(self.cache_index[&#34;lookup_file&#34;]):
            os.remove(self.cache_index[&#34;lookup_file&#34;])
        super().clean()
    
    def write_index_file(self, index_info):
        
        ## add lookup_data to the index
        lookup_file = f&#34;{self.cache_base_path}.lookup&#34;
        
        with open(lookup_file, &#34;wb&#34;) as f:
            pickle.dump(self.lookup_data, f)
        
        index_info[&#34;lookup_file&#34;] = lookup_file
        
        with open(self.cache_index_path, &#34;w&#34;) as f:
            json.dump(index_info, f)
            
    def __build_generator_from_index(self):
        
        self.lookup_data = CachedDataLoaderwLookup._load_lookup_data(self.cache_index[&#34;lookup_file&#34;])
        
        return super().__build_generator_from_index()
    
    
def access_embeddings(func):
    &#34;&#34;&#34;
    A simple decorator function to access the embeddings property if present in the data
    &#34;&#34;&#34;
    def function_wrapper(*args, **kwargs):
        if isinstance(kwargs, dict) and &#34;embeddings&#34; in kwargs:
            return func(**kwargs[&#34;embeddings&#34;])
        else:
            return func(*args, **kwargs)
            
        
    return function_wrapper
    
@access_embeddings
def build_bert_embeddings(checkpoint, bert_layer_index=-1, **kwargs):
    
    assert bert_layer_index &lt; 0
    
    bert_model = TFAutoModel.from_pretrained(checkpoint,
                                             output_attentions = False,
                                             output_hidden_states = bert_layer_index!=-1,
                                             return_dict=True,
                                             from_pt=True)
    
    if bert_layer_index==-1:
        @tf.function
        def embeddings(**kwargs):
            return bert_model(kwargs)
    else:
        # use hidden_states
        @tf.function
        def embeddings(**kwargs):
            out = bert_model(kwargs)
            return TFBaseModelOutputWithPooling(last_hidden_state=out[&#34;hidden_states&#34;][bert_layer_index],
                                                pooler_output=out[&#34;hidden_states&#34;][bert_layer_index][:,0,:])
    
    return embeddings</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="polus.data.access_embeddings"><code class="name flex">
<span>def <span class="ident">access_embeddings</span></span>(<span>func)</span>
</code></dt>
<dd>
<div class="desc"><p>A simple decorator function to access the embeddings property if present in the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def access_embeddings(func):
    &#34;&#34;&#34;
    A simple decorator function to access the embeddings property if present in the data
    &#34;&#34;&#34;
    def function_wrapper(*args, **kwargs):
        if isinstance(kwargs, dict) and &#34;embeddings&#34; in kwargs:
            return func(**kwargs[&#34;embeddings&#34;])
        else:
            return func(*args, **kwargs)
            
        
    return function_wrapper</code></pre>
</details>
</dd>
<dt id="polus.data.build_bert_embeddings"><code class="name flex">
<span>def <span class="ident">build_bert_embeddings</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def function_wrapper(*args, **kwargs):
    if isinstance(kwargs, dict) and &#34;embeddings&#34; in kwargs:
        return func(**kwargs[&#34;embeddings&#34;])
    else:
        return func(*args, **kwargs)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="polus.data.CachedDataLoader"><code class="flex name class">
<span>class <span class="ident">CachedDataLoader</span></span>
<span>(</span><span>source_generator=None, accelerated_map_f=None, accelerated_map_batch=128, show_progress=False, tf_sample_map_f=None, py_sample_map_f=None, do_clean_up=False, cache_additional_identifier='', cache_chunk_size=8192, cache_folder='.polus_cache/data', cache_index=None)</span>
</code></dt>
<dd>
<div class="desc"><p>source_generator
- the source generator that feeds the data
accelerated_map_f
- an optional transformation function that would be applied to
the source generator samples, this function will be executed in the best hardware that tf founds on the device</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CachedDataLoader(DataLoader):
    
    
    def __init__(self, 
                 source_generator = None,
                 accelerated_map_f = None,
                 accelerated_map_batch = 128,
                 show_progress = False,
                 tf_sample_map_f = None, # sample mapping function written and executed in tensorflow that is applied before the data is stored in cache
                 py_sample_map_f = None, # sample mapping function written and executed in python that is applied before the data is stored in cache
                 do_clean_up = False,
                 cache_additional_identifier = &#34;&#34;,
                 cache_chunk_size = 8192,
                 cache_folder=os.path.join(&#34;.polus_cache&#34;,&#34;data&#34;),
                 cache_index = None): # this variable is used to init a CacheDataLoader with an already cached index usefull in merge
        
        # impossible condition
        assert source_generator is not None or cache_index is not None

        self.cache_folder = cache_folder
        self.cache_chunk_size = cache_chunk_size
        self.cache_additional_identifier = cache_additional_identifier
        self.shuffle_blocks = False
        self.do_clean_up = do_clean_up
        self.__tf_sample_map_f = tf_sample_map_f
        self.__py_sample_map_f = py_sample_map_f
        self.cache_index = cache_index
        
        # the parent DataLoader will call _build_sample_generator that contains the logic to build the dataloader
        try:
            super().__init__(source_generator, accelerated_map_f=accelerated_map_f, show_progress=show_progress, accelerated_map_batch=accelerated_map_batch)
        except Exception as e:
            # here we dont want to solve the exception, we just want to clean up de previously created files
            self.logger.info(&#34;An error has occured so all the created files will be deleted&#34;)
            self.clean()
            raise e
            
    @classmethod
    def from_cached_index(cls, index_path):
        
        index_info = cls.read_index(index_path)
        
        return cls(cache_index=index_info)
        
    
    @classmethod
    def merge(cls, *cache_dataloaders):
        assert (len(cache_dataloaders)&gt;1)

        index_info = {&#34;files&#34;:[],
                      &#34;cache_chunk_size&#34;: 0,
                      &#34;n_samples&#34;: 0
                      }
        
        # read files
        for dl in cache_dataloaders:
            
            index = CachedDataLoader.read_index(dl.cache_index_path)
                
            index_info[&#34;n_samples&#34;] += index[&#34;n_samples&#34;]
            index_info[&#34;files&#34;].extend(index[&#34;files&#34;])
            
            # this is a bit starange, but it is possible to have different DL with diff chunk size so we will pick the larger one to define the conjunt
            index_info[&#34;cache_chunk_size&#34;] = max(index_info[&#34;cache_chunk_size&#34;], index[&#34;cache_chunk_size&#34;])
            
        return cls(cache_index=index_info)
                 
    @staticmethod
    def read_index(file_path):
        with open(file_path, &#34;r&#34;) as f:
            index = json.load(f)
        return index
    
    def _build_sample_generator(self, source_generator):
        
        if self.cache_index is not None:
            # we are alredy have the data needed to create the DataLoader
            # So, let&#39;s build it
            return self.__build_generator_from_index()
                 
        if not os.path.exists(self.cache_folder):
            os.makedirs(self.cache_folder)

        # get path to cache
        self.cache_base_name = self.__build_cache_base_name(source_generator)
        self.cache_base_path = os.path.join(self.cache_folder, self.cache_base_name)
        self.cache_index_path = f&#34;{self.cache_base_path}.index&#34;
        
        generator = None
        
        if not os.path.exists(self.cache_index_path):
             # build a generator that reads the files from cache
            self.logger.info(f&#34;DataLoader will store the samples in {self.cache_base_path}, with a max_sample per file of {self.cache_chunk_size}, this may take a while&#34;)
            # there are no history of a previous generator so we must generate the samples once and then store in cache
            # normaly apply the accelerated_map_f to the source_generator
            generator = super()._build_sample_generator(source_generator)
            
            if self.__tf_sample_map_f is not None:
                tf_source_generator = generator
                
                if isinstance(self.__tf_sample_map_f, IAccelerated_Map):
                    # get the map function and run any heavy init only 1 time!
                    self.__tf_sample_map_f = self.__tf_sample_map_f.build()
                
                def generator():
                    
                    dytpes, shapes = find_dtype_and_shapes(tf_source_generator())
                    inner_tf_dataset = tf.data.Dataset.from_generator(tf_source_generator, 
                                                                      output_types= dytpes,
                                                                      output_shapes= shapes)\
                                                      .map(self.__tf_sample_map_f, num_parallel_calls=tf.data.AUTOTUNE)\
                                                      .prefetch(tf.data.AUTOTUNE)

                    for data in inner_tf_dataset:
                        yield data
            
            if self.__py_sample_map_f is not None:
                
                py_source_generator = generator
                
                if isinstance(self.__py_sample_map_f, IAccelerated_Map):
                    # get the map function and run any heavy init only 1 time!
                    self.__py_sample_map_f = self.__py_sample_map_f.build()
                
                def generator():
                    for data in py_source_generator():
                        yield self.__py_sample_map_f(data)
        else:
            self.logger.info(f&#34;We found a compatible cache file for this DataLoader&#34;)
            
        return self._build_cache_generator(generator)
    
    def write_index_file(self, index_info):
        
        with open(self.cache_index_path, &#34;w&#34;) as f:
            json.dump(index_info, f)
    
    def _build_cache_generator(self, generator = None):
        # first its need to store in cache the samples
        if generator is not None:
            
            index_info = {&#34;files&#34;:[],
                          &#34;cache_chunk_size&#34;: self.cache_chunk_size,
                          }
            
            def write_to_file(file_id, data, index):
                
                file_path = f&#34;{self.cache_base_path}_{file_id:04}.part&#34;

                with open(file_path, &#34;wb&#34;) as f:
                    pickle.dump(data, f)
                    
                index[&#34;files&#34;].append(file_path)
            
            _temp_data = []
            _file_index = 0
            n_samples = 0
            
            # TODO: Change the tf section here so that all the tf function launched here can be destroyed
            self.logger.info(&#34;Starting to cache the dataset, this may take a while&#34;)
            
            for data in generator():
                n_samples += 1
                
                _temp_data.append(data)
                if len(_temp_data) &gt;= self.cache_chunk_size:
                    # save data to file
                    
                    write_to_file(_file_index, _temp_data, index_info)
                    
                    # clean up 
                    _temp_data = []
                    _file_index+=1
            
            # TODO: swap to the main tf session and destroy the previous one
            
            if len(_temp_data)&gt;0:
                # save the reminder data
                write_to_file(_file_index, _temp_data, index_info)
            
            index_info[&#34;n_samples&#34;] = n_samples
            
            # write the index file
            self.write_index_file(index_info)
        
        if self.do_clean_up:
            # TODO: make test to see if this thing works
            self.logger.info(&#34;The current tf session will be reseted to clear the computation done by the mapping function&#34;)
            tf.keras.backend.clear_session()
        
        # read the index from file
        self.cache_index = self.__class__.read_index(self.cache_index_path)
        
        # Build the cache generator
        
        return self.__build_generator_from_index()

    
    def clean(self):
        if hasattr(self, &#34;cache_index&#34;) and self.cache_index is not None and &#34;files&#34; in self.cache_index:
            for file in self.cache_index[&#34;files&#34;]:
                if os.path.exists(file):
                    os.remove(file)
        if hasattr(self, &#34;cache_index_path&#34;) and self.cache_index_path is not None and os.path.exists(self.cache_index_path):
            os.remove(self.cache_index_path)
                 
    def __build_generator_from_index(self):
        # set n_samples
        self.n_samples = self.cache_index[&#34;n_samples&#34;]
        self.cache_chunk_size = self.cache_index[&#34;cache_chunk_size&#34;]
                 
        self.logger.info(f&#34;Total number of samples in dataset: {self.n_samples}&#34;)
        
        def generator():
            aux_file_index = list(range(len(self.cache_index[&#34;files&#34;])))
            
            if self.shuffle_blocks:
                random.shuffle(aux_file_index)
                
            for file_index in aux_file_index:
                with open(self.cache_index[&#34;files&#34;][file_index], &#34;rb&#34;) as f:
                    for sample in pickle.load(f):
                        yield sample
                 
        return generator
        
    def __build_cache_base_name(self, source_generator):
        name = &#34;&#34;
        if self.cache_additional_identifier!=&#34;&#34;:
            name = f&#34;{self.cache_additional_identifier}_&#34;
        
        if self.accelerated_map_f is not None:
            name += self.accelerated_map_f.__name__
            
        if self.__tf_sample_map_f is not None:
            name += self.__tf_sample_map_f.__name__
            
        if self.__py_sample_map_f is not None:
            name += self.__py_sample_map_f.__name__
            
        return f&#34;{name}_{source_generator.__name__}&#34;
    
    
    def pre_shuffle(self):
        &#34;&#34;&#34;
        The order of the cached files will be readed in a random order
        &#34;&#34;&#34;
        self.shuffle_blocks = True
        
        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="polus.data.DataLoader" href="#polus.data.DataLoader">DataLoader</a></li>
<li><a title="polus.core.BaseLogger" href="core.html#polus.core.BaseLogger">BaseLogger</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="polus.data.CachedDataLoaderwLookup" href="#polus.data.CachedDataLoaderwLookup">CachedDataLoaderwLookup</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="polus.data.CachedDataLoader.from_cached_index"><code class="name flex">
<span>def <span class="ident">from_cached_index</span></span>(<span>index_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_cached_index(cls, index_path):
    
    index_info = cls.read_index(index_path)
    
    return cls(cache_index=index_info)</code></pre>
</details>
</dd>
<dt id="polus.data.CachedDataLoader.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>*cache_dataloaders)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def merge(cls, *cache_dataloaders):
    assert (len(cache_dataloaders)&gt;1)

    index_info = {&#34;files&#34;:[],
                  &#34;cache_chunk_size&#34;: 0,
                  &#34;n_samples&#34;: 0
                  }
    
    # read files
    for dl in cache_dataloaders:
        
        index = CachedDataLoader.read_index(dl.cache_index_path)
            
        index_info[&#34;n_samples&#34;] += index[&#34;n_samples&#34;]
        index_info[&#34;files&#34;].extend(index[&#34;files&#34;])
        
        # this is a bit starange, but it is possible to have different DL with diff chunk size so we will pick the larger one to define the conjunt
        index_info[&#34;cache_chunk_size&#34;] = max(index_info[&#34;cache_chunk_size&#34;], index[&#34;cache_chunk_size&#34;])
        
    return cls(cache_index=index_info)</code></pre>
</details>
</dd>
<dt id="polus.data.CachedDataLoader.read_index"><code class="name flex">
<span>def <span class="ident">read_index</span></span>(<span>file_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def read_index(file_path):
    with open(file_path, &#34;r&#34;) as f:
        index = json.load(f)
    return index</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="polus.data.CachedDataLoader.clean"><code class="name flex">
<span>def <span class="ident">clean</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean(self):
    if hasattr(self, &#34;cache_index&#34;) and self.cache_index is not None and &#34;files&#34; in self.cache_index:
        for file in self.cache_index[&#34;files&#34;]:
            if os.path.exists(file):
                os.remove(file)
    if hasattr(self, &#34;cache_index_path&#34;) and self.cache_index_path is not None and os.path.exists(self.cache_index_path):
        os.remove(self.cache_index_path)</code></pre>
</details>
</dd>
<dt id="polus.data.CachedDataLoader.pre_shuffle"><code class="name flex">
<span>def <span class="ident">pre_shuffle</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>The order of the cached files will be readed in a random order</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_shuffle(self):
    &#34;&#34;&#34;
    The order of the cached files will be readed in a random order
    &#34;&#34;&#34;
    self.shuffle_blocks = True
    
    return self</code></pre>
</details>
</dd>
<dt id="polus.data.CachedDataLoader.write_index_file"><code class="name flex">
<span>def <span class="ident">write_index_file</span></span>(<span>self, index_info)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_index_file(self, index_info):
    
    with open(self.cache_index_path, &#34;w&#34;) as f:
        json.dump(index_info, f)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="polus.data.CachedDataLoaderwLookup"><code class="flex name class">
<span>class <span class="ident">CachedDataLoaderwLookup</span></span>
<span>(</span><span>lookup_data=None, cache_index=None, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Correspond to a CachedDataLoader but also keeps track of an aditional lookup file</p>
<p>source_generator
- the source generator that feeds the data
accelerated_map_f
- an optional transformation function that would be applied to
the source generator samples, this function will be executed in the best hardware that tf founds on the device</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CachedDataLoaderwLookup(CachedDataLoader):
    &#34;&#34;&#34;
    Correspond to a CachedDataLoader but also keeps track of an aditional lookup file
    &#34;&#34;&#34;
    
    def __init__(self, 
                 lookup_data = None,
                 cache_index=None,  # this variable is used to init a CacheDataLoader with an already cached index usefull in merge
                 *args,
                 **kwargs,
                ):
        
        if cache_index is not None and &#34;lookup_file&#34; in cache_index:
            with open(cache_index[&#34;lookup_file&#34;], &#34;rb&#34;) as f:
                lookup_data = pickle.load(f)
        
        if lookup_data is None:
            raise ValueError(&#34;Do not use CachedDataLoaderwLookup without setting a lookup_data, instead use CachedDataLoader&#34;)
        
        self.lookup_data = lookup_data
        
        # the parent DataLoader will call _build_sample_generator that contains the logic to build the dataloader
        super().__init__(*args, cache_index=cache_index, **kwargs)
        
    def get_lookup_data(self):
        return self.lookup_data
    
    @staticmethod
    def _load_lookup_data(lookup_file):
        with open(lookup_file, &#34;rb&#34;) as f:
            return pickle.load(f)
    
    @classmethod
    def merge(cls, *cache_dataloaders):
        assert (len(cache_dataloaders)&gt;1)

        index_info = {&#34;files&#34;:[],
                      &#34;cache_chunk_size&#34;: 0,
                      &#34;n_samples&#34;: 0
                      }
        
        lookup_data = []
        
        # read files
        for dl in cache_dataloaders:
            
            index = CachedDataLoaderwLookup.read_index(dl.cache_index_path)
                
            index_info[&#34;n_samples&#34;] += index[&#34;n_samples&#34;]
            index_info[&#34;files&#34;].extend(index[&#34;files&#34;])
            
            # this is a bit starange, but it is possible to have different DL with diff chunk size so we will pick the larger one to define the conjunt
            index_info[&#34;cache_chunk_size&#34;] = max(index_info[&#34;cache_chunk_size&#34;], index[&#34;cache_chunk_size&#34;])
            
            lookup_data.extend(CachedDataLoaderwLookup._load_lookup_data(index[&#34;lookup_file&#34;]))
            
        return CachedDataLoaderwLookup(cache_index=index_info, lookup_data=lookup_data)
    
    def clean(self):

        if hasattr(self, &#34;cache_index&#34;) and self.cache_index is not None and &#34;lookup_file&#34; in self.cache_index and os.path.exists(self.cache_index[&#34;lookup_file&#34;]):
            os.remove(self.cache_index[&#34;lookup_file&#34;])
        super().clean()
    
    def write_index_file(self, index_info):
        
        ## add lookup_data to the index
        lookup_file = f&#34;{self.cache_base_path}.lookup&#34;
        
        with open(lookup_file, &#34;wb&#34;) as f:
            pickle.dump(self.lookup_data, f)
        
        index_info[&#34;lookup_file&#34;] = lookup_file
        
        with open(self.cache_index_path, &#34;w&#34;) as f:
            json.dump(index_info, f)
            
    def __build_generator_from_index(self):
        
        self.lookup_data = CachedDataLoaderwLookup._load_lookup_data(self.cache_index[&#34;lookup_file&#34;])
        
        return super().__build_generator_from_index()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="polus.data.CachedDataLoader" href="#polus.data.CachedDataLoader">CachedDataLoader</a></li>
<li><a title="polus.data.DataLoader" href="#polus.data.DataLoader">DataLoader</a></li>
<li><a title="polus.core.BaseLogger" href="core.html#polus.core.BaseLogger">BaseLogger</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="polus.data.CachedDataLoaderwLookup.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>*cache_dataloaders)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def merge(cls, *cache_dataloaders):
    assert (len(cache_dataloaders)&gt;1)

    index_info = {&#34;files&#34;:[],
                  &#34;cache_chunk_size&#34;: 0,
                  &#34;n_samples&#34;: 0
                  }
    
    lookup_data = []
    
    # read files
    for dl in cache_dataloaders:
        
        index = CachedDataLoaderwLookup.read_index(dl.cache_index_path)
            
        index_info[&#34;n_samples&#34;] += index[&#34;n_samples&#34;]
        index_info[&#34;files&#34;].extend(index[&#34;files&#34;])
        
        # this is a bit starange, but it is possible to have different DL with diff chunk size so we will pick the larger one to define the conjunt
        index_info[&#34;cache_chunk_size&#34;] = max(index_info[&#34;cache_chunk_size&#34;], index[&#34;cache_chunk_size&#34;])
        
        lookup_data.extend(CachedDataLoaderwLookup._load_lookup_data(index[&#34;lookup_file&#34;]))
        
    return CachedDataLoaderwLookup(cache_index=index_info, lookup_data=lookup_data)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="polus.data.CachedDataLoaderwLookup.clean"><code class="name flex">
<span>def <span class="ident">clean</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean(self):

    if hasattr(self, &#34;cache_index&#34;) and self.cache_index is not None and &#34;lookup_file&#34; in self.cache_index and os.path.exists(self.cache_index[&#34;lookup_file&#34;]):
        os.remove(self.cache_index[&#34;lookup_file&#34;])
    super().clean()</code></pre>
</details>
</dd>
<dt id="polus.data.CachedDataLoaderwLookup.get_lookup_data"><code class="name flex">
<span>def <span class="ident">get_lookup_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_lookup_data(self):
    return self.lookup_data</code></pre>
</details>
</dd>
<dt id="polus.data.CachedDataLoaderwLookup.write_index_file"><code class="name flex">
<span>def <span class="ident">write_index_file</span></span>(<span>self, index_info)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_index_file(self, index_info):
    
    ## add lookup_data to the index
    lookup_file = f&#34;{self.cache_base_path}.lookup&#34;
    
    with open(lookup_file, &#34;wb&#34;) as f:
        pickle.dump(self.lookup_data, f)
    
    index_info[&#34;lookup_file&#34;] = lookup_file
    
    with open(self.cache_index_path, &#34;w&#34;) as f:
        json.dump(index_info, f)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="polus.data.CachedDataLoader" href="#polus.data.CachedDataLoader">CachedDataLoader</a></b></code>:
<ul class="hlist">
<li><code><a title="polus.data.CachedDataLoader.pre_shuffle" href="#polus.data.CachedDataLoader.pre_shuffle">pre_shuffle</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="polus.data.DataLoader"><code class="flex name class">
<span>class <span class="ident">DataLoader</span></span>
<span>(</span><span>source_generator, accelerated_map_f=None, accelerated_map_batch=128, show_progress=False)</span>
</code></dt>
<dd>
<div class="desc"><p>source_generator
- the source generator that feeds the data
accelerated_map_f
- an optional transformation function that would be applied to
the source generator samples, this function will be executed in the best hardware that tf founds on the device</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataLoader(BaseLogger):
    
    def __init__(self, 
                 source_generator,
                 accelerated_map_f = None,
                 accelerated_map_batch = 128,
                 show_progress = False):
        &#34;&#34;&#34;
        source_generator         - the source generator that feeds the data
        accelerated_map_f        - an optional transformation function that would be applied to 
                                   the source generator samples, this function will be executed in the best hardware that tf founds on the device
        
        &#34;&#34;&#34;
        super().__init__()
        
        self.show_progress = show_progress
        self.accelerated_map_batch = accelerated_map_batch
        self.accelerated_map_f = accelerated_map_f
        
        self.sample_generator = self._build_sample_generator(source_generator)

        dytpes, shapes = find_dtype_and_shapes(self.sample_generator())
        
        self.tf_dataset = tf.data.Dataset.from_generator(self.sample_generator, 
                                                         output_types= dytpes,
                                                         output_shapes= shapes)
            
        # expose all the tf dataset methods
        for method in filter(lambda x: not x[0].startswith(&#34;_&#34;), inspect.getmembers(self.tf_dataset, predicate=inspect.ismethod)):
            setattr(self, method[0], method[1])
    
    
    def _build_sample_generator(self, source_generator):
        
        # Logic to construct a generator that maybe applies the accelerated_map_f to the source_generator
        if self.accelerated_map_f is None:
            generator = source_generator
        
        else:
            # build and store the accelerated_map_f
            if isinstance(self.accelerated_map_f, IAccelerated_Map):
                # get the map function and run any heavy init only 1 time!
                self.accelerated_map_f = self.accelerated_map_f.build()
                        
            def generator():
                
                # BATCH = 128
                dytpes, shapes = find_dtype_and_shapes(source_generator())
                inner_tf_dataset = tf.data.Dataset.from_generator(source_generator, 
                                                                  output_types= dytpes,
                                                                  output_shapes= shapes)\
                                                  .batch(self.accelerated_map_batch)\
                                                  .prefetch(10)
                
                for i, data in enumerate(inner_tf_dataset):
                    
                    if self.show_progress:
                        print(f&#34;Iteration: {i*self.accelerated_map_batch}&#34;, end=&#34;\r&#34;)
                    
                    start_bert_t = timer()
                    data = self.accelerated_map_f(data)
                    self.logger.debug(f&#34;Time taken to run BERT {timer()-start_bert_t}&#34;)
                    ## debatching in python
                    ## TODO: this may be a bottleneck since python is slow
                    start_unbatch_t = timer()
                    if isinstance(data, dict):
                        key = list(data.keys())[0]
                        n_samples = data[key].shape[0] #batch size
                        for i in range(n_samples):
                            yield { k:v[i] for k, v in data.items() } 
                    elif isinstance(data, (list,tuple)):
                        n_samples = data[0].shape[0]
                        for i in range(n_samples):
                            yield [ v[i] for v in data ] 
                    else:
                        raise ValueError(f&#34;the accelerated_map_f function does not yield a dict nor tuple nor a list&#34;)
                    self.logger.debug(f&#34;Time taken to unbatch {timer()-start_unbatch_t}&#34;)
            
        return generator

    def __iter__(self):
        return self.tf_dataset.__iter__()

    def get_n_samples(self):
        if hasattr(self, &#34;n_samples&#34;):
            return self.n_samples
        else:
            self.logger.info(&#34;this dataset does not have the number of samples in cache so it will take some time to counting&#34;)
            n_samples = 0
            for _ in self.tf_dataset:
                n_samples += 1
                
            self.n_samples = n_samples
            
            return self.n_samples</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="polus.core.BaseLogger" href="core.html#polus.core.BaseLogger">BaseLogger</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="polus.data.CachedDataLoader" href="#polus.data.CachedDataLoader">CachedDataLoader</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="polus.data.DataLoader.get_n_samples"><code class="name flex">
<span>def <span class="ident">get_n_samples</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_n_samples(self):
    if hasattr(self, &#34;n_samples&#34;):
        return self.n_samples
    else:
        self.logger.info(&#34;this dataset does not have the number of samples in cache so it will take some time to counting&#34;)
        n_samples = 0
        for _ in self.tf_dataset:
            n_samples += 1
            
        self.n_samples = n_samples
        
        return self.n_samples</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="polus.data.IAccelerated_Map"><code class="flex name class">
<span>class <span class="ident">IAccelerated_Map</span></span>
</code></dt>
<dd>
<div class="desc"><p>From: <a href="https://www.toptal.com/python/in-depth-python-logging">https://www.toptal.com/python/in-depth-python-logging</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IAccelerated_Map(BaseLogger):
    def __init__(self):
        super().__init__()
        
        self.__name__ = self.__class__.__name__
        
        if self.__class__.__name__ == &#34;IAccelerated_Map&#34;:
            raise Exception(&#34;This is an interface that cannot be instantiated&#34;)
            
    def build(self):
        raise (&#34;build method was not implemented&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="polus.core.BaseLogger" href="core.html#polus.core.BaseLogger">BaseLogger</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="polus.data.IAccelerated_Map.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self):
    raise (&#34;build method was not implemented&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="polus" href="index.html">polus</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="polus.data.access_embeddings" href="#polus.data.access_embeddings">access_embeddings</a></code></li>
<li><code><a title="polus.data.build_bert_embeddings" href="#polus.data.build_bert_embeddings">build_bert_embeddings</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="polus.data.CachedDataLoader" href="#polus.data.CachedDataLoader">CachedDataLoader</a></code></h4>
<ul class="two-column">
<li><code><a title="polus.data.CachedDataLoader.clean" href="#polus.data.CachedDataLoader.clean">clean</a></code></li>
<li><code><a title="polus.data.CachedDataLoader.from_cached_index" href="#polus.data.CachedDataLoader.from_cached_index">from_cached_index</a></code></li>
<li><code><a title="polus.data.CachedDataLoader.merge" href="#polus.data.CachedDataLoader.merge">merge</a></code></li>
<li><code><a title="polus.data.CachedDataLoader.pre_shuffle" href="#polus.data.CachedDataLoader.pre_shuffle">pre_shuffle</a></code></li>
<li><code><a title="polus.data.CachedDataLoader.read_index" href="#polus.data.CachedDataLoader.read_index">read_index</a></code></li>
<li><code><a title="polus.data.CachedDataLoader.write_index_file" href="#polus.data.CachedDataLoader.write_index_file">write_index_file</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="polus.data.CachedDataLoaderwLookup" href="#polus.data.CachedDataLoaderwLookup">CachedDataLoaderwLookup</a></code></h4>
<ul class="">
<li><code><a title="polus.data.CachedDataLoaderwLookup.clean" href="#polus.data.CachedDataLoaderwLookup.clean">clean</a></code></li>
<li><code><a title="polus.data.CachedDataLoaderwLookup.get_lookup_data" href="#polus.data.CachedDataLoaderwLookup.get_lookup_data">get_lookup_data</a></code></li>
<li><code><a title="polus.data.CachedDataLoaderwLookup.merge" href="#polus.data.CachedDataLoaderwLookup.merge">merge</a></code></li>
<li><code><a title="polus.data.CachedDataLoaderwLookup.write_index_file" href="#polus.data.CachedDataLoaderwLookup.write_index_file">write_index_file</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="polus.data.DataLoader" href="#polus.data.DataLoader">DataLoader</a></code></h4>
<ul class="">
<li><code><a title="polus.data.DataLoader.get_n_samples" href="#polus.data.DataLoader.get_n_samples">get_n_samples</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="polus.data.IAccelerated_Map" href="#polus.data.IAccelerated_Map">IAccelerated_Map</a></code></h4>
<ul class="">
<li><code><a title="polus.data.IAccelerated_Map.build" href="#polus.data.IAccelerated_Map.build">build</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>